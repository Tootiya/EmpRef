{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41c528f8",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Intsall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2885ba93",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install --upgrade typing_extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674fffc1",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install --upgrade datasets transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e06311b",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ec430c-3c11-46a5-86a2-a3a45713b509",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40a4a7a-b12b-4b41-a8f6-c29b2677f609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "df = pd.read_csv('empref_df_cleaned.csv')\n",
    "\n",
    "def check_and_prepare_data(df):\n",
    "    if isinstance(df['text'].iloc[0], list):\n",
    "        df['text'] = df['text'].apply(lambda x: ' '.join(x))\n",
    "    return df\n",
    "\n",
    "df = check_and_prepare_data(df)\n",
    "\n",
    "df['er'] = df['er'].fillna(0).astype(int)\n",
    "df['in'] = df['in'].fillna(0).astype(int)\n",
    "df['ex'] = df['ex'].fillna(0).astype(int)\n",
    "\n",
    "dialact_dict = {\n",
    "    0: \"acknowledging\",\n",
    "    1: \"agreeing\",\n",
    "    2: \"consoling\",\n",
    "    3: \"encouraging\",\n",
    "    4: \"questioning\",\n",
    "    5: \"sympathizing\",\n",
    "    6: \"wishing\",\n",
    "    7: \"neutral/suggesting\"\n",
    "}\n",
    "emotion_dict = {\n",
    "    0: \"admiration/love/pride/gratitude\",\n",
    "    1: \"anger/annoyance/disgust/disapproval\",\n",
    "    2: \"approval/optimism\",\n",
    "    3: \"caring/desire\",\n",
    "    4: \"fear/nervousness\",\n",
    "    5: \"joy/amusement/excitement/relief\",\n",
    "    6: \"sadness/disappointment/embarrassment/grief/remorse\",\n",
    "    7: \"surprise/confusion/curiosity/realization\",\n",
    "    8: \"neutral\"\n",
    "}\n",
    "\n",
    "def preprocess_for_llama2_chat(df):\n",
    "    training_examples = []\n",
    "\n",
    "    for dialog_id in df['dialog_id'].unique():\n",
    "        dialog_df = df[df['dialog_id'] == dialog_id]\n",
    "\n",
    "        sys_rows = dialog_df[dialog_df['con/res'] == 'sys']\n",
    "\n",
    "        for index, sys_row in sys_rows.iterrows():\n",
    "            context_df = dialog_df.loc[:index-1]\n",
    "            \n",
    "            context = ' '.join(\n",
    "                f\"<{row['author'].capitalize()}>: (emotion: {emotion_dict[row['emotion']]}, intent: {dialact_dict[row['dialact']]}) {row['text']}\"\n",
    "                for _, row in context_df.iterrows()\n",
    "            )\n",
    "            \n",
    "            response = (\n",
    "                f\"<{sys_row['author'].capitalize()}>: (emotion: {emotion_dict[sys_row['emotion']]}, intent: {dialact_dict[sys_row['dialact']]}, \"\n",
    "                f\"er: {sys_row['er']}, in: {sys_row['in']}, ex: {sys_row['ex']}) {sys_row['text']}\"\n",
    "            )\n",
    "            \n",
    "            formatted_input = f\"<s>[INST]{context}[/INST] {response}</s>\"\n",
    "            training_examples.append(formatted_input)\n",
    "\n",
    "    return training_examples\n",
    "\n",
    "formatted_training_data = preprocess_for_llama2_chat(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c877158b-e880-470a-b0b4-c6e04abf5790",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\"text\": formatted_training_data}\n",
    "full_dataset = Dataset.from_dict(data_dict)\n",
    "\n",
    "train_dataset, temp_dataset = full_dataset.train_test_split(test_size=0.15, seed=42).values()\n",
    "valid_dataset, test_dataset = temp_dataset.train_test_split(test_size=1/3, seed=42).values()\n",
    "\n",
    "prepend_datasets = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'valid': valid_dataset,\n",
    "    'test': test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe09030a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(prepend_datasets['train']), len(prepend_datasets['valid']), len(prepend_datasets['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05269ba5-4161-48b6-bbf2-1f7b7488d909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_llama2_chat_aug(df):\n",
    "    training_examples = []\n",
    "\n",
    "    for dialog_id in df['dialog_id'].unique():\n",
    "        dialog_df = df[df['dialog_id'] == dialog_id]\n",
    "\n",
    "        sys_rows = dialog_df[dialog_df['con/res'] == 'sys']\n",
    "\n",
    "        for index, sys_row in sys_rows.iterrows():\n",
    "\n",
    "            context_df = dialog_df.loc[:index-1]\n",
    "            \n",
    "\n",
    "            context = ' '.join(f\"<{row['author'].capitalize()}>: {row['text']}\" for _, row in context_df.iterrows())\n",
    "            \n",
    "\n",
    "            response = f\"<{sys_row['author'].capitalize()}>: {sys_row['text']}\"\n",
    "            \n",
    "\n",
    "            formatted_input = f\"<s>[INST]{context}[/INST] {response}</s>\"\n",
    "            training_examples.append(formatted_input)\n",
    "\n",
    "    return training_examples\n",
    "\n",
    "formatted_training_data_aug = preprocess_for_llama2_chat_aug(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0ef546-114a-42a6-b7f4-132b9343d66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "data_dict = {\n",
    "    \"text\": formatted_training_data\n",
    "}\n",
    "full_dataset = Dataset.from_dict(data_dict)\n",
    "\n",
    "\n",
    "train_dataset, temp_dataset = full_dataset.train_test_split(test_size=0.15, seed=42).values()\n",
    "\n",
    "\n",
    "valid_dataset, test_dataset = temp_dataset.train_test_split(test_size=1/3, seed=42).values()\n",
    "\n",
    "datasets = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'valid': valid_dataset,\n",
    "    'test': test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d2241f",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c4f505-eb0e-45dd-8c76-72d825334cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(datasets['test']), len(prepend_datasets['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9edbd7-5cc0-45ef-b010-ff4cfb922315",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_conversation_context(sample):\n",
    "    start = sample.find(\"[INST]\") + len(\"[INST]\")\n",
    "    end = sample.find(\"[/INST]\")\n",
    "    return sample[start:end].strip()\n",
    "\n",
    "\n",
    "def extract_target_response(sample):\n",
    "    end = sample.find(\"[/INST]\") + len(\"[/INST]\")\n",
    "    return sample[end:].strip()\n",
    "\n",
    "\n",
    "def generate_response(model, tokenizer, context, max_new_tokens=100):\n",
    "    input_text = f\"<s>[INST]{context}[/INST]\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(inputs['input_ids'], max_new_tokens=max_new_tokens, num_return_sequences=1)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Extract the response part correctly after [/INST] and truncate after </s>\n",
    "def extract_response(generated_text):\n",
    "    end_inst = generated_text.find(\"[/INST]\") + len(\"[/INST]\")\n",
    "    if end_inst == -1:\n",
    "        return \"\"\n",
    "    response_start = generated_text[end_inst:].strip()\n",
    "    end_s = response_start.find(\"</s>\")\n",
    "    if end_s != -1:\n",
    "        response_start = response_start[:end_s].strip()\n",
    "    return response_start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e139151c",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e2fe61-1c10-42f6-956c-b687fd780564",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load base model and tokenizer\n",
    "base_model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_name, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load fine-tuned model\n",
    "fine_tuned_model_name = \"llama-2-7b-reflection-finetuned\"\n",
    "fine_tuned_model = AutoModelForCausalLM.from_pretrained(fine_tuned_model_name, device_map=\"auto\")\n",
    "\n",
    "# Load prepend model\n",
    "prepend_model_name = \"llama-2-7b-reflection-prepend\"\n",
    "prepend_model = AutoModelForCausalLM.from_pretrained(prepend_model_name, device_map=\"auto\")\n",
    "\n",
    "# Load EmpRef model\n",
    "prepend_model_name = \"llama-2-7b-reflection-empref\"\n",
    "prepend_model = AutoModelForCausalLM.from_pretrained(prepend_model_name, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e70d7e-321c-4295-9830-c382134bb34a",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345fa33b-3a97-4bc4-ac2c-945ef3eff5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.meteor_score import single_meteor_score\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Randomly sample 20 examples from the datasets\n",
    "random.seed(42)  # For reproducibility\n",
    "sample_indices = random.sample(range(len(datasets['test'])), 60)\n",
    "\n",
    "# Initialize dictionaries to hold generated responses\n",
    "generated_responses = {\n",
    "    \"base\": [],\n",
    "    \"fine_tuned\": [],\n",
    "    \"fine_tuned_with_prepend\": [],\n",
    "    \"prepend\": []\n",
    "}\n",
    "\n",
    "# Initialize list to hold target responses\n",
    "target_responses = {\n",
    "    \"base_and_fine_tuned\": [],\n",
    "    \"prepend\": []\n",
    "}\n",
    "\n",
    "# Iterate over the sampled examples\n",
    "for idx in sample_indices:\n",
    "    datasets_test_sample = datasets['test'][idx]['text']\n",
    "    prepend_datasets_test_sample = prepend_datasets['test'][idx]['text']\n",
    "\n",
    "    # Extract conversation contexts\n",
    "    context_base_and_finetuned = extract_conversation_context(datasets_test_sample)\n",
    "    context_prepend_and_empref = extract_conversation_context(prepend_datasets_test_sample)\n",
    "\n",
    "    # Extract target responses\n",
    "    target_response_base_and_finetuned = extract_target_response(datasets_test_sample)\n",
    "    target_response_prepend = extract_target_response(prepend_datasets_test_sample)\n",
    "\n",
    "    # Store the target responses\n",
    "    target_responses[\"base_and_fine_tuned\"].append(target_response_base_and_finetuned)\n",
    "    target_responses[\"prepend_and_empref\"].append(target_response_prepend)\n",
    "\n",
    "    # Generate and extract responses\n",
    "    base_response = generate_response(base_model, tokenizer, context_base_and_finetuned)\n",
    "    fine_tuned_response = generate_response(fine_tuned_model, tokenizer, context_base_and_finetuned)\n",
    "    prepend_response = generate_response(prepend_model, tokenizer, context_prepend_and_empref)\n",
    "    prepend_response = generate_response(empref_model, tokenizer, context_prepend_and_empref)\n",
    "\n",
    "    # Extract the generated response part\n",
    "    base_response_extracted = extract_response(base_response)\n",
    "    fine_tuned_response_extracted = extract_response(fine_tuned_response)\n",
    "    prepend_response_extracted = extract_response(prepend_response)\n",
    "    empref_prepend_response_extracted = extract_response(empref_response)\n",
    "\n",
    "    # Store the generated responses\n",
    "    generated_responses[\"base\"].append(base_response_extracted)\n",
    "    generated_responses[\"fine_tuned\"].append(fine_tuned_response_extracted)\n",
    "    generated_responses[\"prepend\"].append(fine_tuned_with_prepend_response_extracted)\n",
    "    generated_responses[\"empref\"].append(prepend_response_extracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342fd0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path\n",
    "output_file_path = 'model_responses_comparison.txt'\n",
    "\n",
    "# Open the file for writing\n",
    "with open(output_file_path, 'w') as file:\n",
    "    # Iterate over the sampled examples and their corresponding responses\n",
    "    for idx in range(len(sample_indices)):\n",
    "        datasets_test_sample = datasets['test'][sample_indices[idx]]['text']\n",
    "        prepend_datasets_test_sample = prepend_datasets['test'][sample_indices[idx]]['text']\n",
    "\n",
    "        context_base_and_finetuned = extract_conversation_context(datasets_test_sample)\n",
    "        context_prepend_and_empref = extract_conversation_context(prepend_datasets_test_sample)\n",
    "\n",
    "        target_response_base_and_finetuned = target_responses[\"base_and_fine_tuned\"][idx]\n",
    "        target_response_prepend = target_responses[\"prepend_and_empref\"][idx]\n",
    "\n",
    "        base_response_extracted = generated_responses[\"base\"][idx]\n",
    "        fine_tuned_response_extracted = generated_responses[\"fine_tuned\"][idx]\n",
    "        prepend_response_extracted = generated_responses[\"prepend\"][idx]\n",
    "        empref_response_extracted = generated_responses[\"empref\"][idx]\n",
    "\n",
    "        # Write to file\n",
    "        file.write(f\"Sample {idx + 1}:\\n\")\n",
    "        file.write(\"Conversation Context:\\n\")\n",
    "        file.write(context_base_and_finetuned + \"\\n\\n\")\n",
    "        \n",
    "        file.write(\"Base Model Response:\\n\")\n",
    "        file.write(base_response_extracted + \"\\n\\n\")\n",
    "        \n",
    "        file.write(\"Fine-tuned Model Response:\\n\")\n",
    "        file.write(fine_tuned_response_extracted + \"\\n\\n\")\n",
    "        \n",
    "        file.write(\"Prepend Input Response:\\n\")\n",
    "        file.write(prepend_response_extracted + \"\\n\\n\")\n",
    "        \n",
    "        file.write(\"EmpRef Model Response:\\n\")\n",
    "        file.write(empref_response_extracted + \"\\n\\n\")\n",
    "        \n",
    "        file.write(\"Target Response:\\n\")\n",
    "        if method in [\"base\", \"fine_tuned\"]:\n",
    "            file.write(target_response_base_and_finetuned + \"\\n\\n\")\n",
    "        else:\n",
    "            file.write(target_response_prepend_and_empref + \"\\n\\n\")\n",
    "        \n",
    "        file.write(\"=\"*80 + \"\\n\\n\")\n",
    "\n",
    "print(f\"Responses have been written to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fba8ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.meteor_score import single_meteor_score\n",
    "\n",
    "# Initialize metrics\n",
    "metrics = {\n",
    "    \"bleu_1\": {method: [] for method in generated_responses},\n",
    "    \"bleu_2\": {method: [] for method in generated_responses},\n",
    "    \"bleu_3\": {method: [] for method in generated_responses},\n",
    "    \"bleu_4\": {method: [] for method in generated_responses},\n",
    "    \"rouge_l\": {method: [] for method in generated_responses},\n",
    "    \"meteor\": {method: [] for method in generated_responses},\n",
    "    \"dist_1\": {method: [] for method in generated_responses},\n",
    "    \"dist_2\": {method: [] for method in generated_responses},\n",
    "}\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "smoothing_function = SmoothingFunction().method1\n",
    "\n",
    "# Function to calculate BLEU scores\n",
    "def calculate_bleu_scores(ref, cand):\n",
    "    ref, cand = ref.split(), cand.split()\n",
    "    bleu_1 = sentence_bleu([ref], cand, weights=(1, 0, 0, 0), smoothing_function=smoothing_function)\n",
    "    bleu_2 = sentence_bleu([ref], cand, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothing_function)\n",
    "    bleu_3 = sentence_bleu([ref], cand, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smoothing_function)\n",
    "    bleu_4 = sentence_bleu([ref], cand, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing_function)\n",
    "    return bleu_1, bleu_2, bleu_3, bleu_4\n",
    "\n",
    "# Function to calculate ROUGE-L score\n",
    "def calculate_rouge_l_score(ref, cand):\n",
    "    scores = scorer.score(ref, cand)\n",
    "    return scores['rougeL'].fmeasure\n",
    "\n",
    "# Function to calculate METEOR score\n",
    "def calculate_meteor_score(ref, cand):\n",
    "    return single_meteor_score(' '.join(ref.split()), ' '.join(cand.split()))\n",
    "\n",
    "# Function to calculate Distinct-1 and Distinct-2 scores\n",
    "def calculate_distinct_scores(cand):\n",
    "    unigrams = cand.split()\n",
    "    bigrams = list(zip(unigrams[:-1], unigrams[1:]))\n",
    "    dist_1 = len(set(unigrams)) / len(unigrams) if len(unigrams) != 0 else 0\n",
    "    dist_2 = len(set(bigrams)) / len(bigrams) if len(bigrams) != 0 else 0\n",
    "    return dist_1, dist_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffec6203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate each method\n",
    "for method, responses in generated_responses.items():\n",
    "    if method in [\"base\", \"fine_tuned\"]:\n",
    "        target_list = target_responses[\"base_and_fine_tuned\"]\n",
    "    else:\n",
    "        target_list = target_responses[\"prepend_and_empref\"]\n",
    "        \n",
    "    for ref, cand in zip(target_list, responses):\n",
    "        # BLEU\n",
    "        bleu_1, bleu_2, bleu_3, bleu_4 = calculate_bleu_scores(ref, cand)\n",
    "        metrics[\"bleu_1\"][method].append(bleu_1)\n",
    "        metrics[\"bleu_2\"][method].append(bleu_2)\n",
    "        metrics[\"bleu_3\"][method].append(bleu_3)\n",
    "        metrics[\"bleu_4\"][method].append(bleu_4)\n",
    "        \n",
    "        # ROUGE-L\n",
    "        metrics[\"rouge_l\"][method].append(calculate_rouge_l_score(ref, cand))\n",
    "        \n",
    "        # METEOR\n",
    "        metrics[\"meteor\"][method].append(calculate_meteor_score(ref, cand))\n",
    "        \n",
    "        # Distinct-1 and Distinct-2\n",
    "        dist_1, dist_2 = calculate_distinct_scores(cand)\n",
    "        metrics[\"dist_1\"][method].append(dist_1)\n",
    "        metrics[\"dist_2\"][method].append(dist_2)\n",
    "\n",
    "# Calculate mean and error bars\n",
    "final_metrics = {}\n",
    "for metric, values in metrics.items():\n",
    "    final_metrics[metric] = {}\n",
    "    for method, scores in values.items():\n",
    "        final_metrics[metric][method] = {\n",
    "            \"mean\": np.mean(scores),\n",
    "            \"std_err\": np.std(scores) / np.sqrt(len(scores))\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911a3801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results\n",
    "for method in generated_responses:\n",
    "    print(f\"Method: {method}\")\n",
    "    print(f\"BLEU-1: {final_metrics['bleu_1'][method]['mean']} ± {final_metrics['bleu_1'][method]['std_err']}\")\n",
    "    print(f\"BLEU-2: {final_metrics['bleu_2'][method]['mean']} ± {final_metrics['bleu_2'][method]['std_err']}\")\n",
    "    print(f\"BLEU-3: {final_metrics['bleu_3'][method]['mean']} ± {final_metrics['bleu_3'][method]['std_err']}\")\n",
    "    print(f\"BLEU-4: {final_metrics['bleu_4'][method]['mean']} ± {final_metrics['bleu_4'][method]['std_err']}\")\n",
    "    print(f\"ROUGE-L: {final_metrics['rouge_l'][method]['mean']} ± {final_metrics['rouge_l'][method]['std_err']}\")\n",
    "    print(f\"METEOR: {final_metrics['meteor'][method]['mean']} ± {final_metrics['meteor'][method]['std_err']}\")\n",
    "    print(f\"Distinct-1: {final_metrics['dist_1'][method]['mean']} ± {final_metrics['dist_1'][method]['std_err']}\")\n",
    "    print(f\"Distinct-2: {final_metrics['dist_2'][method]['mean']} ± {final_metrics['dist_2'][method]['std_err']}\")\n",
    "    print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
