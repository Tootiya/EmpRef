{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e15c4c90-15b9-4150-9f32-6df4776d02bc",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bbde733-ee24-4ba1-8595-c277126cfcdb",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: accelerate in /home/ubuntu/.local/lib/python3.10/site-packages (0.30.1)\n",
      "Requirement already satisfied: peft in /home/ubuntu/.local/lib/python3.10/site-packages (0.11.1)\n",
      "Requirement already satisfied: bitsandbytes in /home/ubuntu/.local/lib/python3.10/site-packages (0.43.1)\n",
      "Requirement already satisfied: transformers in /home/ubuntu/.local/lib/python3.10/site-packages (4.41.2)\n",
      "Requirement already satisfied: trl in /home/ubuntu/.local/lib/python3.10/site-packages (0.9.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from accelerate) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from accelerate) (21.3)\n",
      "Requirement already satisfied: psutil in /usr/lib/python3/dist-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from accelerate) (5.4.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/lib/python3/dist-packages (from accelerate) (2.0.1)\n",
      "Requirement already satisfied: huggingface-hub in /home/ubuntu/.local/lib/python3.10/site-packages (from accelerate) (0.23.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from accelerate) (0.4.3)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/.local/lib/python3.10/site-packages (from peft) (4.66.1)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: datasets in /home/ubuntu/.local/lib/python3.10/site-packages (from trl) (2.19.2)\n",
      "Requirement already satisfied: tyro>=0.5.11 in /home/ubuntu/.local/lib/python3.10/site-packages (from trl) (0.8.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.8.0)\n",
      "Requirement already satisfied: docstring-parser>=0.14.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (0.16)\n",
      "Requirement already satisfied: rich>=11.1.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (13.7.1)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /home/ubuntu/.local/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (1.7.1)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets->trl) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets->trl) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets->trl) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/lib/python3/dist-packages (from datasets->trl) (1.3.5)\n",
      "Requirement already satisfied: xxhash in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets->trl) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets->trl) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets->trl) (3.9.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->datasets->trl) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->datasets->trl) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->datasets->trl) (4.0.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n",
      "\u001b[33mDEPRECATION: flatbuffers 1.12.1-git20200711.33e2d80-dfsg1-0.6 has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of flatbuffers or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install accelerate peft bitsandbytes transformers trl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a1c806-f41f-4e55-8d93-7b404db437de",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5be5c15b-7f6a-43a9-862a-260bcfcb9828",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-07 08:05:38.142461: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-07 08:05:38.691929: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c087f1e6-c02c-437d-90ff-fb64c368744c",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "065d9cc2-e229-476a-ad16-181dbf800027",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('empref_df_cleaned.csv')\n",
    "\n",
    "def check_and_prepare_data(df):\n",
    "    if isinstance(df['text'].iloc[0], list):\n",
    "        df['text'] = df['text'].apply(lambda x: ' '.join(x))\n",
    "    return df\n",
    "\n",
    "df = check_and_prepare_data(df)\n",
    "\n",
    "df['er'] = df['er'].fillna(0).astype(int)\n",
    "df['in'] = df['in'].fillna(0).astype(int)\n",
    "df['ex'] = df['ex'].fillna(0).astype(int)\n",
    "\n",
    "dialact_dict = {\n",
    "    0: \"acknowledging\",\n",
    "    1: \"agreeing\",\n",
    "    2: \"consoling\",\n",
    "    3: \"encouraging\",\n",
    "    4: \"questioning\",\n",
    "    5: \"sympathizing\",\n",
    "    6: \"wishing\",\n",
    "    7: \"neutral/suggesting\"\n",
    "}\n",
    "emotion_dict = {\n",
    "    0: \"admiration/love/pride/gratitude\",\n",
    "    1: \"anger/annoyance/disgust/disapproval\",\n",
    "    2: \"approval/optimism\",\n",
    "    3: \"caring/desire\",\n",
    "    4: \"fear/nervousness\",\n",
    "    5: \"joy/amusement/excitement/relief\",\n",
    "    6: \"sadness/disappointment/embarrassment/grief/remorse\",\n",
    "    7: \"surprise/confusion/curiosity/realization\",\n",
    "    8: \"neutral\"\n",
    "}\n",
    "\n",
    "def preprocess_for_llama2_chat(df):\n",
    "    training_examples = []\n",
    "\n",
    "    for dialog_id in df['dialog_id'].unique():\n",
    "        dialog_df = df[df['dialog_id'] == dialog_id]\n",
    "\n",
    "        sys_rows = dialog_df[dialog_df['con/res'] == 'sys']\n",
    "\n",
    "        for index, sys_row in sys_rows.iterrows():\n",
    "            context_df = dialog_df.loc[:index-1]\n",
    "            \n",
    "            context = ' '.join(\n",
    "                f\"<{row['author'].capitalize()}>: (emotion: {emotion_dict[row['emotion']]}, intent: {dialact_dict[row['dialact']]}) {row['text']}\"\n",
    "                for _, row in context_df.iterrows()\n",
    "            )\n",
    "            \n",
    "            response = (\n",
    "                f\"<{sys_row['author'].capitalize()}>: (emotion: {emotion_dict[sys_row['emotion']]}, intent: {dialact_dict[sys_row['dialact']]}, \"\n",
    "                f\"er: {sys_row['er']}, in: {sys_row['in']}, ex: {sys_row['ex']}) {sys_row['text']}\"\n",
    "            )\n",
    "            \n",
    "            formatted_input = f\"<s>[INST]{context}[/INST] {response}</s>\"\n",
    "            training_examples.append(formatted_input)\n",
    "\n",
    "    return training_examples\n",
    "\n",
    "formatted_training_data = preprocess_for_llama2_chat(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf0e5103-6a98-4ad6-b6da-46e72ab3fa50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<s>[INST]<Speaker>: (emotion: approval/optimism, intent: questioning) I'm going through some things with my feelings and myself. I barely sleep and I do nothing but think about how I'm worthless and how I shouldn't be here.\\n   I've never tried or contemplated suicide. I've always wanted to fix my issues, but I never get around to it.\\n   How can I change my feeling of being worthless to everyone? <Listener>: (emotion: admiration/love/pride/gratitude, intent: sympathizing) Hello, and thank you for your question and seeking advice on this. <Listener>: (emotion: sadness/disappointment/embarrassment/grief/remorse, intent: agreeing) Feelings of worthlessness is unfortunately common. <Listener>: (emotion: neutral, intent: agreeing) In fact, most people, if not all, have felt this to some degree at some point in their life. <Listener>: (emotion: neutral, intent: agreeing) You are not alone. <Listener>: (emotion: neutral, intent: neutral/suggesting) Changing our feelings is like changing our thoughts - it's hard to do. <Listener>: (emotion: admiration/love/pride/gratitude, intent: neutral/suggesting) Our minds are so amazing that the minute you change your thought another one can be right there to take it's place. <Listener>: (emotion: neutral, intent: neutral/suggesting) Without your permission, another thought can just pop in there. <Listener>: (emotion: neutral, intent: neutral/suggesting) The new thought may feel worse than the last one![/INST] <Listener>: (emotion: neutral, intent: neutral/suggesting, er: 0, in: 0, ex: 0) My guess is that you have tried several things to improve this on your own even before reaching out on here.</s>\",\n",
       " '<s>[INST]<Speaker>: (emotion: admiration/love/pride/gratitude, intent: wishing) Hello everyone, I love this sub but have never posted and today I feel a need to. One example of this is the other day, my husband went out to lunch with his mom and my son while she was at work. Have a wonderful day.[/INST] <Listener>: (emotion: neutral, intent: questioning, er: 1, in: 0, ex: 0) You are in a tough situation.</s>',\n",
       " \"<s>[INST]<Speaker>: (emotion: approval/optimism, intent: questioning) I'm going through some things with my feelings and myself. I barely sleep and I do nothing but think about how I'm worthless and how I shouldn't be here.\\n   I've never tried or contemplated suicide. I've always wanted to fix my issues, but I never get around to it.\\n   How can I change my feeling of being worthless to everyone?[/INST] <Listener>: (emotion: neutral, intent: acknowledging, er: 0, in: 0, ex: 0) That is intense.</s>\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_training_data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62a761a9-018d-4b53-ad20-29bd63f9c400",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\"text\": formatted_training_data}\n",
    "full_dataset = Dataset.from_dict(data_dict)\n",
    "\n",
    "train_dataset, temp_dataset = full_dataset.train_test_split(test_size=0.15, seed=42).values()\n",
    "valid_dataset, test_dataset = temp_dataset.train_test_split(test_size=1/3, seed=42).values()\n",
    "\n",
    "datasets = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'valid': valid_dataset,\n",
    "    'test': test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d063ec-9013-4ca1-916b-8f5ae328440b",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92116c2c-6fc9-48b6-83de-c47dfc696c7e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfa97d57-e2ae-4599-ab53-fbf26cae24fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80aa687c617f445bbc08f4e425359d64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6075f6-1eec-4232-9125-bf5b6fad0713",
   "metadata": {},
   "source": [
    "## loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e7d8de0-32bd-485c-97b2-49ec20c388bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09eae059bc824e7384a8465fd4402d3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "base_model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "refined_model = \"llama-2-7b-reflection-prepend\" #You can give it your own name\n",
    "\n",
    "# Tokenizer\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "llama_tokenizer.padding_side = \"right\"  # Fix for fp16\n",
    "\n",
    "# # Quantization Config\n",
    "# quant_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.float16,\n",
    "#     bnb_4bit_use_double_quant=False\n",
    "# )\n",
    "\n",
    "# Model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    # quantization_config=quant_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "base_model.config.use_cache = False\n",
    "base_model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "708c5164-c3f3-4a07-9ecb-78ee14d93f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/tootiya/myenv/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:159: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bdbc124f9b54af89505407c45438822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1572 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e9d0e34f7a64ea69d2980b5884365f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/185 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/tootiya/myenv/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1965' max='1965' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1965/1965 37:27, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.747095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.993700</td>\n",
       "      <td>0.613119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.672100</td>\n",
       "      <td>0.507770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.410300</td>\n",
       "      <td>0.464325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.410300</td>\n",
       "      <td>0.447785</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LoRA Config\n",
    "peft_parameters = LoraConfig(\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    r=16,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Training Params\n",
    "train_params = TrainingArguments(\n",
    "    output_dir=\"./results_prepend\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    # optim=\"paged_adamw_32bit\",\n",
    "    # save_steps=200,\n",
    "    # logging_steps=100,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    load_best_model_at_end = True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "    learning_rate=5e-4, #2e-4,\n",
    "    weight_decay= 0.01, #0.001,*\n",
    "    fp16=True,\n",
    "    # bf16=False,\n",
    "    # per_device_eval_batch_size=8,\n",
    "    # max_grad_norm=0.3,\n",
    "    # max_steps=-1,\n",
    "    warmup_ratio=0.01,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type= \"linear\", # \"constant\",\n",
    "    report_to=\"tensorboard\",\n",
    "    # group_by_length=True,  # Group by length for efficiency\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "fine_tuning = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=datasets['train'],\n",
    "    eval_dataset=datasets['valid'],\n",
    "    peft_config=peft_parameters,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=llama_tokenizer,\n",
    "    args=train_params,)\n",
    "\n",
    "fine_tuning.train()\n",
    "\n",
    "fine_tuning.model.save_pretrained(refined_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d8ba69-437b-43a5-81af-a2af6abc4c17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
