{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bec56bbd-dd6b-43e7-a8e0-9b8f9f351e2d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbf00c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file_path = 'empref_df_final_full.csv'\n",
    "dataset = pd.read_csv(file_path)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f384e336",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ecfa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cleaned = dataset.drop(columns=['Unnamed: 0', 'index', 'speaker_name'])\n",
    "\n",
    "dataset_cleaned = dataset_cleaned.drop(columns=['segment_id'])\n",
    "\n",
    "dataset_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4740dd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b25f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'empref_df_cleaned.csv'\n",
    "dataset_cleaned.to_csv(save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e09f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dialogs = dataset_cleaned['dialog_id'].nunique()\n",
    "print(total_dialogs)\n",
    "\n",
    "# 2. Maximum and Average Turn Count\n",
    "max_turn_count = dataset_cleaned['turn_count'].max()\n",
    "avg_turn_count = dataset_cleaned['turn_count'].mean()\n",
    "print(max_turn_count, avg_turn_count)\n",
    "\n",
    "# 3. Average Turn Number for 'sys' Entries\n",
    "avg_turn_sys = dataset_cleaned[dataset_cleaned['con/res'] == 'sys']['turn'].mean()\n",
    "max_turn_sys = dataset_cleaned[dataset_cleaned['con/res'] == 'sys']['turn'].max()\n",
    "print(max_turn_sys, avg_turn_sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3138d36a-2731-4a9a-b83e-7a86ac655181",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cleaned = pd.read_csv('empref_df_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79caade3-d654-47b0-ace7-391c23fed573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the dictionaries\n",
    "dialact_dict = {\n",
    "    0: \"acknowledging\",\n",
    "    1: \"agreeing\",\n",
    "    2: \"consoling\",\n",
    "    3: \"encouraging\",\n",
    "    4: \"questioning\",\n",
    "    5: \"sympathizing\",\n",
    "    6: \"wishing\",\n",
    "    7: \"neutral/suggesting\"\n",
    "}\n",
    "\n",
    "emotion_dict = {\n",
    "    0: \"admiration/love/pride/gratitude\",\n",
    "    1: \"anger/annoyance/disgust/disapproval\",\n",
    "    2: \"approval/optimism\",\n",
    "    3: \"caring/desire\",\n",
    "    4: \"fear/nervousness\",\n",
    "    5: \"joy/amusement/excitement/relief\",\n",
    "    6: \"sadness/disappointment/embarrassment/grief/remorse\",\n",
    "    7: \"surprise/confusion/curiosity/realization\",\n",
    "    8: \"neutral\"\n",
    "}\n",
    "\n",
    "dataset_cleaned['dialact_text'] = dataset_cleaned['dialact'].map(dialact_dict)\n",
    "dataset_cleaned['emotion_text'] = dataset_cleaned['emotion'].map(emotion_dict)\n",
    "\n",
    "dataset_cleaned['role'] = dataset_cleaned.apply(\n",
    "    lambda row: 'sys' if row['con/res'] == 'sys' else ('listener' if row['author'] == 'listener' else 'speaker'),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "def compute_normalized_counts(df, category):\n",
    "    counts = df.groupby(['role', category]).size().reset_index(name='count')\n",
    "    total_counts = counts.groupby('role')['count'].sum().reset_index(name='total_count')\n",
    "    merged_counts = counts.merge(total_counts, on='role')\n",
    "    merged_counts['proportion'] = merged_counts['count'] / merged_counts['total_count']\n",
    "    return merged_counts\n",
    "\n",
    "normalized_dialact = compute_normalized_counts(dataset_cleaned, 'dialact_text')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(y='dialact_text', x='proportion', hue='role', data=normalized_dialact, ci=None)\n",
    "plt.title('Normalized Distribution of Dialact Values by Role')\n",
    "plt.xlabel('Proportion')\n",
    "plt.ylabel('Dialact')\n",
    "plt.legend(title='Role')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "normalized_emotion = compute_normalized_counts(dataset_cleaned, 'emotion_text')\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(y='emotion_text', x='proportion', hue='role', data=normalized_emotion, ci=None)\n",
    "plt.title('Normalized Distribution of Emotion Values by Role')\n",
    "plt.xlabel('Proportion')\n",
    "plt.ylabel('Emotion')\n",
    "plt.legend(title='Role')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018c9b83",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c650d5-2130-4c3a-be60-f01a01218d70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    AutoConfig\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers.models.llama.modeling_llama import LlamaForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd00560",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c66765",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('empref_df_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed18c1aa-cdf8-41b1-b66a-45af71f5fcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3938bbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "author_counts = df['author'].value_counts()\n",
    "\n",
    "con_res_counts = df['con/res'].value_counts()\n",
    "\n",
    "author_counts, con_res_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f59f77-1967-4877-a2af-0702c0c5638f",
   "metadata": {},
   "source": [
    "## base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a553d37-2770-4304-b0f3-d0650ed99663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_llama2_chat(df):\n",
    "    training_examples = []\n",
    "\n",
    "    for dialog_id in df['dialog_id'].unique():\n",
    "        dialog_df = df[df['dialog_id'] == dialog_id]\n",
    "\n",
    "        # Filter out rows where 'con/res' is 'sys', indicating a target response\n",
    "        sys_rows = dialog_df[dialog_df['con/res'] == 'sys']\n",
    "\n",
    "        for index, sys_row in sys_rows.iterrows():\n",
    "            # Get all preceding context up to (but not including) the current sys row\n",
    "            context_df = dialog_df.loc[:index-1]\n",
    "            \n",
    "            # Format the context with author tags\n",
    "            context = ' '.join(f\"<{row['author'].capitalize()}>: {row['text']}\" for _, row in context_df.iterrows())\n",
    "            \n",
    "            # Prepare the target response\n",
    "            response = f\"<{sys_row['author'].capitalize()}>: {sys_row['text']}\"\n",
    "            \n",
    "            # Format according to LLaMA-2-chat expected input\n",
    "            formatted_input = f\"<s>[INST]{context}[/INST] {response}</s>\"\n",
    "            training_examples.append(formatted_input)\n",
    "\n",
    "    return training_examples\n",
    "\n",
    "\n",
    "formatted_training_data = preprocess_for_llama2_chat(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d187f560-04bf-4c93-a7fa-3c700a119cf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "formatted_training_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22924f7-aa6c-4347-9e98-06ad551987aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(formatted_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e898f4-494c-4dce-ab67-05262d05e52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "data_dict = {\n",
    "    \"text\": formatted_training_data  # Ensure this is a list of strings\n",
    "}\n",
    "full_dataset = Dataset.from_dict(data_dict)\n",
    "\n",
    "train_dataset, temp_dataset = full_dataset.train_test_split(test_size=0.15, seed=42).values()\n",
    "\n",
    "valid_dataset, test_dataset = temp_dataset.train_test_split(test_size=1/3, seed=42).values()\n",
    "\n",
    "datasets = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'valid': valid_dataset,\n",
    "    'test': test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a103e0-25e0-4427-96ed-225521e9bc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d54447-e39c-4ade-b6e2-ef77c4418f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets['valid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388fa869-5789-467c-b346-014a847a4fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ccf398-5fc1-457c-9d2d-f661c081d60b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## EmpRef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bc939d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "# Define the custom model with device_map=\"auto\"\n",
    "base_model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "llama_tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb2c23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('empref_df_cleaned.csv')\n",
    "​\n",
    "def check_and_prepare_data(df):\n",
    "    if isinstance(df['text'].iloc[0], list):\n",
    "        df['text'] = df['text'].apply(lambda x: ' '.join(x))\n",
    "    return df\n",
    "\n",
    "df = check_and_prepare_data(df)\n",
    "df['er'] = df['er'].fillna(0).astype(int)\n",
    "df['in'] = df['in'].fillna(0).astype(int)\n",
    "df['ex'] = df['ex'].fillna(0).astype(int)\n",
    "\n",
    "dialact_dict = {\n",
    "    0: \"acknowledging\",\n",
    "    1: \"agreeing\",\n",
    "    2: \"consoling\",\n",
    "    3: \"encouraging\",\n",
    "    4: \"questioning\",\n",
    "    5: \"sympathizing\",\n",
    "    6: \"wishing\",\n",
    "    7: \"neutral/suggesting\"\n",
    "}\n",
    "emotion_dict = {\n",
    "    0: \"admiration/love/pride/gratitude\",\n",
    "    1: \"anger/annoyance/disgust/disapproval\",\n",
    "    2: \"approval/optimism\",\n",
    "    3: \"caring/desire\",\n",
    "    4: \"fear/nervousness\",\n",
    "    5: \"joy/amusement/excitement/relief\",\n",
    "    6: \"sadness/disappointment/embarrassment/grief/remorse\",\n",
    "    7: \"surprise/confusion/curiosity/realization\",\n",
    "    8: \"neutral\"\n",
    "}\n",
    "\n",
    "# Preprocess the data to include all necessary labels\n",
    "def preprocess_for_custom_model(df, tokenizer):\n",
    "    def get_labels(row):\n",
    "        return {\n",
    "            \"intent_ids\": row['dialact'],\n",
    "            \"emotion_ids\": row['emotion'],\n",
    "            \"er_ids\": row['er'],\n",
    "            \"in_ids\": row['in'],\n",
    "            \"ex_ids\": row['ex']\n",
    "        }\n",
    "​\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    for dialog_id in df['dialog_id'].unique():\n",
    "        dialog_df = df[df['dialog_id'] == dialog_id]\n",
    "        sys_rows = dialog_df[dialog_df['con/res'] == 'sys']\n",
    "​\n",
    "        for index, sys_row in sys_rows.iterrows():\n",
    "            context_df = dialog_df.loc[:index - 1]\n",
    "            context = ' '.join(\n",
    "                f\"<{row['author'].capitalize()}>: (emotion: {emotion_dict[row['emotion']]}, intent: {dialact_dict[row['dialact']]}) {row['text']}\"\n",
    "                for _, row in context_df.iterrows()\n",
    "            )\n",
    "            response = (\n",
    "                f\"<{sys_row['author'].capitalize()}>: (emotion: {emotion_dict[sys_row['emotion']]}, intent: {dialact_dict[sys_row['dialact']]}, \"\n",
    "                f\"er: {sys_row['er']}, in: {sys_row['in']}, ex: {sys_row['ex']}) {sys_row['text']}\"\n",
    "            )\n",
    "​\n",
    "            formatted_input = f\"<s>[INST]{context}[/INST] {response}</s>\"\n",
    "            tokenized_input = tokenizer(formatted_input, return_tensors='pt', padding=True, truncation=True)\n",
    "            inputs.append({\n",
    "                \"input_ids\": tokenized_input[\"input_ids\"].squeeze(0),\n",
    "                \"attention_mask\": tokenized_input[\"attention_mask\"].squeeze(0),\n",
    "                \"text\": formatted_input\n",
    "            })\n",
    "            labels.append(get_labels(sys_row))\n",
    "​\n",
    "    dataset = Dataset.from_dict({\n",
    "        \"input_ids\": [x[\"input_ids\"] for x in inputs],\n",
    "        \"attention_mask\": [x[\"attention_mask\"] for x in inputs],\n",
    "        \"text\": [x[\"text\"] for x in inputs],\n",
    "        \"intent_ids\": [x[\"intent_ids\"] for x in labels],\n",
    "        \"emotion_ids\": [x[\"emotion_ids\"] for x in labels],\n",
    "        \"er_ids\": [x[\"er_ids\"] for x in labels],\n",
    "        \"in_ids\": [x[\"in_ids\"] for x in labels],\n",
    "        \"ex_ids\": [x[\"ex_ids\"] for x in labels]\n",
    "    })\n",
    "    return dataset\n",
    "\n",
    "formatted_training_data = preprocess_for_custom_model(df, llama_tokenizer)\n",
    "​\n",
    "# Splitting the dataset\n",
    "train_dataset, temp_dataset = formatted_training_data.train_test_split(test_size=0.15, seed=42).values()\n",
    "valid_dataset, test_dataset = temp_dataset.train_test_split(test_size=1/3, seed=42).values()\n",
    "​\n",
    "datasets = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'valid': valid_dataset,\n",
    "    'test': test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29de08cc",
   "metadata": {},
   "source": [
    "# loading base model and tokenizer from HF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72353064-b002-42fa-af03-e5b76cec133a",
   "metadata": {},
   "source": [
    "## base model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f462bc27-67a7-4eb7-9de1-c109736f5382",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13aa82d9-5432-48d7-be62-c5705b39e835",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "base_model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "#refined_model = \"llama-2-7b-reflection-finetuned\" #You can give it your own name\n",
    "\n",
    "# Tokenizer\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "llama_tokenizer.padding_side = \"right\"  # Fix for fp16\n",
    "\n",
    "# # Quantization Config\n",
    "# quant_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.float16,\n",
    "#     bnb_4bit_use_double_quant=False\n",
    "# )\n",
    "\n",
    "# Model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    # quantization_config=quant_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "base_model.config.use_cache = False\n",
    "base_model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8774788d-51e8-4982-9482-4e3bede038f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.save_pretrained('./llama_local')\n",
    "llama_tokenizer.save_pretrained('./llama_local')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfb6ba3-a21d-4905-a0e7-1630745b6f45",
   "metadata": {},
   "source": [
    "# push to hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e50e349-3f37-4114-99e4-71f422a2f65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "load_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(load_model, refined_model)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4a3b19-d1c1-4718-ac32-dc13082a9c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(refined_model, use_temp_dir=False)\n",
    "tokenizer.push_to_hub(refined_model, use_temp_dir=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed49d9d3-6489-44e3-807b-3106df2b6efc",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d0a959-8100-4d9c-b1ba-2e7dad8bb774",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "refined_model = \"llama-2-7b-reflection-finetuned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f33e546-9642-455b-afe8-6fca464f678d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fine_tuned_model = AutoModelForCausalLM.from_pretrained(refined_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69e2cbb-fb97-437d-b99a-3b6e961754da",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fine_tuned_tokenizer = AutoTokenizer.from_pretrained(refined_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340f4867-a01d-4986-a524-86c7f6e8a454",
   "metadata": {},
   "source": [
    "# Training with LoRa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bf2141-b2e0-468e-a433-0d73098af273",
   "metadata": {},
   "source": [
    "## Base model fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ed5243-405f-42a7-9592-05b5578a7829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA Config\n",
    "peft_parameters = LoraConfig(\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    r=16,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Training Params\n",
    "train_params = TrainingArguments(\n",
    "    output_dir=\"./results_finetuned\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    # optim=\"paged_adamw_32bit\",\n",
    "    # save_steps=200,\n",
    "    # logging_steps=100,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    load_best_model_at_end = True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "    learning_rate=5e-4, #2e-4,\n",
    "    weight_decay= 0.01, #0.001,*\n",
    "    fp16=True,\n",
    "    # bf16=False,\n",
    "    # per_device_eval_batch_size=8,\n",
    "    # max_grad_norm=0.3,\n",
    "    # max_steps=-1,\n",
    "    warmup_ratio=0.01,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type= \"linear\", # \"constant\",\n",
    "    report_to=\"tensorboard\",\n",
    "    # group_by_length=True,  # Group by length for efficiency\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "fine_tuning = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=datasets['train'],\n",
    "    eval_dataset=datasets['valid'],\n",
    "    peft_config=peft_parameters,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=llama_tokenizer,\n",
    "    args=train_params,)\n",
    "\n",
    "# Training\n",
    "fine_tuning.train()\n",
    "\n",
    "# Save Model\n",
    "fine_tuning.model.save_pretrained(refined_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8d3c3c-6bfc-4030-9d07-b357f90929ec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## EmpRef fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4243669d-1dd6-4964-be31-2a7e73bdd287",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLLaMA(LlamaForCausalLM):\n",
    "    def __init__(self, config, num_intents, num_emotions, embedding_dim):\n",
    "        super().__init__(config)\n",
    "        self.intent_embeddings = nn.Embedding(num_intents, embedding_dim)\n",
    "        self.emotion_embeddings = nn.Embedding(num_emotions, embedding_dim)\n",
    "        enhanced_dim = config.hidden_size + 2 * embedding_dim\n",
    "        self.emotion_head = nn.Linear(enhanced_dim, num_emotions)\n",
    "        self.intent_head = nn.Linear(enhanced_dim, num_intents)\n",
    "        self.er_head = nn.Linear(config.hidden_size, 2)\n",
    "        self.ex_head = nn.Linear(config.hidden_size, 2)\n",
    "        self.in_head = nn.Linear(config.hidden_size, 2)\n",
    "\n",
    "        self.criterion_nll = nn.CrossEntropyLoss()\n",
    "        self.criterion_aux = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None, intent_ids=None, emotion_ids=None, er_ids=None, in_ids=None, ex_ids=None):\n",
    "        outputs = super().forward(input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "        hidden_states = outputs.hidden_states[-1]\n",
    "\n",
    "        if labels is not None:\n",
    "            intent_embeds = self.intent_embeddings(intent_ids)\n",
    "            emotion_embeds = self.emotion_embeddings(emotion_ids)\n",
    "            enhanced_hidden_states = torch.cat((hidden_states, intent_embeds, emotion_embeds), dim=-1)\n",
    "\n",
    "            emotion_logits = self.emotion_head(enhanced_hidden_states)\n",
    "            intent_logits = self.intent_head(enhanced_hidden_states)\n",
    "            er_logits = self.er_head(hidden_states)\n",
    "            in_logits = self.in_head(hidden_states)\n",
    "            ex_logits = self.ex_head(hidden_states)\n",
    "\n",
    "            total_loss = (\n",
    "                self.criterion_aux(emotion_logits, emotion_ids) +\n",
    "                self.criterion_aux(intent_logits, intent_ids) +\n",
    "                self.criterion_aux(er_logits, er_ids) +\n",
    "                self.criterion_aux(in_logits, in_ids) +\n",
    "                self.criterion_aux(ex_logits, ex_ids)\n",
    "            )\n",
    "            return {\n",
    "                \"loss\": total_loss,\n",
    "                \"emotion_logits\": emotion_logits,\n",
    "                \"intent_logits\": intent_logits,\n",
    "                \"er_logits\": er_logits,\n",
    "                \"in_logits\": in_logits,\n",
    "                \"ex_logits\": ex_logits\n",
    "            }\n",
    "        else:\n",
    "            return {\"hidden_states\": hidden_states}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cbd6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(base_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2969c418",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model = CustomLLaMA(\n",
    "    config=config,\n",
    "    num_intents=9,\n",
    "    num_emotions=9,\n",
    "    embedding_dim=50\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e4ddd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer\n",
    "class CustomSFTTrainer(SFTTrainer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.model = kwargs['model']\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        intent_ids = inputs.pop(\"intent_ids\")\n",
    "        emotion_ids = inputs.pop(\"emotion_ids\")\n",
    "        er_ids = inputs.pop(\"er_ids\")\n",
    "        in_ids = inputs.pop(\"in_ids\")\n",
    "        ex_ids = inputs.pop(\"ex_ids\")\n",
    "\n",
    "        outputs = model(**inputs, labels=labels, intent_ids=intent_ids, emotion_ids=emotion_ids, er_ids=er_ids, in_ids=in_ids, ex_ids=ex_ids)\n",
    "        loss = outputs[\"loss\"]\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4991312b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LoRA Config\n",
    "peft_parameters = LoraConfig(\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    r=16,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_empref\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "    learning_rate=5e-4,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    warmup_ratio=0.01,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "fine_tuning = CustomSFTTrainer(\n",
    "    model=custom_model,\n",
    "    train_dataset=datasets['train'],\n",
    "    eval_dataset=datasets['valid'],\n",
    "    peft_config=peft_parameters,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=llama_tokenizer,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "# Training\n",
    "fine_tuning.train()\n",
    "\n",
    "# Save Model\n",
    "refined_model = \"llama-2-7b-reflection-empref\"\n",
    "fine_tuning.model.save_pretrained(refined_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
